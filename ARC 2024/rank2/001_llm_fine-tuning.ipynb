{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune an LLM to learn to count objects in a grid, or to solve ARC tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I might do 2 steps of fine-tuning:\n",
    "\n",
    "1. Learning priors, f.e. learning to count\n",
    "2. Solve ARC tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/ironbar/prompt_recovery/blob/main/notebooks/012_fine-tune_llama.ipynb\n",
    "- https://github.com/ironbar/prompt_recovery/blob/main/notebooks/020_fine-tune_final_ensemble.ipynb\n",
    "- https://www.kaggle.com/code/ironbar/few-shot-prompting-for-arc24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# from abc import ABC, abstractmethod\n",
    "# import numpy as np\n",
    "# from termcolor import colored\n",
    "# from tqdm.auto import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib as mpl\n",
    "# from matplotlib import colors\n",
    "# import wandb\n",
    "from typing import Optional\n",
    "\n",
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, pipeline\n",
    "# from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "# from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "# from datasets import Dataset\n",
    "\n",
    "# plt.plot()\n",
    "# plt.close('all')\n",
    "# plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "# mpl.rcParams['lines.linewidth'] = 3\n",
    "# mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    model_path = \"/home/gbarbadillo/data/Phi-3-mini-128k-instruct\"\n",
    "    adapter_path: Optional[str] = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/15_continue_training_phi3_4e5/checkpoint-22800' # Set it to None to train lora from scratch\n",
    "    train_dataset = '/mnt/hdd0/Kaggle/arc24/data/learn_to_count/learn_to_count_100000.json'\n",
    "    val_dataset = '/mnt/hdd0/Kaggle/arc24/data/learn_to_count/learn_to_count_1000.json'\n",
    "    output_dir = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/22_random_question_lr1e-4_1e5dataset'\n",
    "    max_seq_len = 512\n",
    "    epochs = 1\n",
    "    eval_steps = 100\n",
    "    warmup_ratio = 0.1\n",
    "    learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    # model_path = \"/home/gbarbadillo/data/llama-3-transformers-8b-chat-hf-v1\"\n",
    "    model_path = '/home/gbarbadillo/data/llama-3.1-transformers-8b-instruct-v1'\n",
    "    adapter_path: Optional[str] = None\n",
    "    # adapter_path: Optional[str] = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/12_llama_lr_2e-4_1e4dataset_r32/checkpoint-600'\n",
    "    use_rslora = False,\n",
    "    use_dora = False,\n",
    "    train_dataset = '/mnt/hdd0/Kaggle/arc24/data/learn_to_count/learn_to_count_100000.json'\n",
    "    val_dataset = '/mnt/hdd0/Kaggle/arc24/data/learn_to_count/learn_to_count_1000.json'\n",
    "    output_dir = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/22_llama31_lr1e-4_1e5dataset_r32'\n",
    "    max_seq_len = 640\n",
    "    epochs = 1\n",
    "    eval_steps = 100\n",
    "    warmup_ratio = 0.1\n",
    "    learning_rate = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "with open(os.path.join(cfg.output_dir, 'cfg.json'), 'w') as f:\n",
    "    json.dump({key:value for key, value in cfg.__dict__.items() if not key.startswith('__')}, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'llama' in cfg.model_path:\n",
    "    device_map = {\n",
    "        'model.embed_tokens': 0,\n",
    "        'model.layers.0': 0,\n",
    "        'model.layers.1': 0,\n",
    "        'model.layers.2': 0,\n",
    "        'model.layers.3': 0,\n",
    "        'model.layers.4': 0,\n",
    "        'model.layers.5': 0,\n",
    "        'model.layers.6': 0,\n",
    "        'model.layers.7': 0,\n",
    "        'model.layers.8': 0,\n",
    "        'model.layers.9': 0,\n",
    "        'model.layers.10': 0,\n",
    "        'model.layers.11': 0,\n",
    "        'model.layers.12': 0,\n",
    "        'model.layers.13': 0,\n",
    "        'model.layers.14': 0,\n",
    "        'model.layers.15': 0,\n",
    "        'model.layers.16': 0,\n",
    "        'model.layers.17': 1,\n",
    "        'model.layers.18': 1,\n",
    "        'model.layers.19': 1,\n",
    "        'model.layers.20': 1,\n",
    "        'model.layers.21': 1,\n",
    "        'model.layers.22': 1,\n",
    "        'model.layers.23': 1,\n",
    "        'model.layers.24': 1,\n",
    "        'model.layers.25': 1,\n",
    "        'model.layers.26': 1,\n",
    "        'model.layers.27': 1,\n",
    "        'model.layers.28': 1,\n",
    "        'model.layers.29': 1,\n",
    "        'model.layers.30': 1,\n",
    "        'model.layers.31': 1,\n",
    "        'model.norm': 1,\n",
    "        'model.rotary_emb': 1,\n",
    "        'lm_head': 1,\n",
    "    }\n",
    "else:\n",
    "    device_map = 'balanced'\n",
    "\n",
    "# device_map = 'balanced'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.model_path,\n",
    "    #quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    # max_memory={0: '9GB', 1: '8GB'},\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cfg.model_path,\n",
    "    trust_remote_code=True)\n",
    "if 'llama' in cfg.model_path:\n",
    "    print('Adding <|pad|> token to tokenizer')\n",
    "    tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    tokenizer.padding_side = 'right'\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    for device in range(torch.cuda.device_count()):\n",
    "        print(f'GPU {device} memory allocated: {torch.cuda.memory_allocated(device)/1024**3:.1f} GB, max memory allocated: {torch.cuda.max_memory_allocated(device)/1024**3:.1f} GB')\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEncoder(ABC):\n",
    "    @abstractmethod\n",
    "    def to_text(self, grid):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def to_grid(self, text):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_grid = np.eye(3, dtype=int).tolist()\n",
    "\n",
    "def test_translator(translator):\n",
    "    assert sample_grid == translator.to_grid(translator.to_text(sample_grid))\n",
    "    print(translator.to_text(sample_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimalGridEncoder(GridEncoder):\n",
    "    @staticmethod\n",
    "    def to_text(grid):\n",
    "        text = '\\n'.join([''.join([str(x) for x in line]) for line in grid])\n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_grid(text):\n",
    "        lines = text.strip().splitlines()\n",
    "        grid = [[int(x) for x in line] for line in lines]\n",
    "        return grid\n",
    "        \n",
    "test_translator(MinimalGridEncoder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWithSeparationEncoder(GridEncoder):\n",
    "    def __init__(self, split_symbol):\n",
    "        self.split_symbol = split_symbol\n",
    "\n",
    "    def to_text(self, grid):\n",
    "        text = '\\n'.join([self.split_symbol.join([str(x) for x in line]) for line in grid])\n",
    "        return text\n",
    "\n",
    "    def to_grid(self, text):\n",
    "        lines = text.strip().splitlines()\n",
    "        grid = [[int(x) for x in line.split(self.split_symbol)] for line in lines]\n",
    "        return grid\n",
    "\n",
    "test_translator(GridWithSeparationEncoder('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridCodeBlockEncoder(GridEncoder):\n",
    "    def __init__(self, base_encoder):\n",
    "        self.encoder = base_encoder\n",
    "\n",
    "    def to_text(self, grid):\n",
    "        text = f'```grid\\n{self.encoder.to_text(grid)}\\n```'\n",
    "        return text\n",
    "\n",
    "    def to_grid(self, text):\n",
    "        grid_text = text.split('```grid\\n')[1].split('\\n```')[0]\n",
    "        grid = self.encoder.to_grid(grid_text)\n",
    "        return grid\n",
    "\n",
    "test_translator(GridCodeBlockEncoder(MinimalGridEncoder()))\n",
    "\n",
    "test_translator(GridCodeBlockEncoder(GridWithSeparationEncoder('|')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(filepath, grid_encoder, shuffle_question_order=True):\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    prompts = []\n",
    "\n",
    "    for sample_id, sample in tqdm(data.items(), total=len(data)):\n",
    "        messages = create_messages_from_sample(\n",
    "            sample, grid_encoder, shuffle_question_order=shuffle_question_order)\n",
    "        prompt = tokenizer.apply_chat_template(messages,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=False)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    np.random.shuffle(prompts)\n",
    "    pretty_print_prompt(prompts[0])\n",
    "\n",
    "    prompt_lengths = [len(tokenizer.encode(prompt)) for prompt in tqdm(prompts)]\n",
    "    plt.hist(prompt_lengths, bins=100);\n",
    "    plt.title('Prompt length distribution')\n",
    "    plt.xlabel('Number of tokens');\n",
    "    plt.show()\n",
    "\n",
    "    prompts = [prompt for prompt, prompt_length in zip(prompts, prompt_lengths) if prompt_length < cfg.max_seq_len]\n",
    "    print(f'Using {len(prompts)} prompts after removing those longer than {cfg.max_seq_len} tokens')\n",
    "\n",
    "    dataset = Dataset.from_dict({'text': prompts})\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_messages_from_sample(sample, grid_encoder, shuffle_question_order=False):\n",
    "    first_message = True\n",
    "    messages = [{'role': 'system', 'content': 'You are a helpful AI assistant'}]\n",
    "    questions = list(sample['questions'].keys())\n",
    "    if shuffle_question_order:\n",
    "        np.random.shuffle(questions)\n",
    "    for question in questions:\n",
    "        answer = sample['questions'][question]\n",
    "        if first_message:\n",
    "            content = grid_encoder.to_text(sample['grid']) + '\\n' + question\n",
    "            first_message = False\n",
    "        else:\n",
    "            content = question\n",
    "        messages.append({'role': 'user', 'content': content})\n",
    "        messages.append({'role': 'assistant', 'content': str(answer)})\n",
    "    return messages\n",
    "\n",
    "\n",
    "def pretty_print_prompt(text, default_color='white'):\n",
    "    color = default_color\n",
    "    attrs = None\n",
    "    for line in text.splitlines():\n",
    "        if line.startswith('<|assistant|>'):\n",
    "            color = 'blue'\n",
    "        elif line.startswith('<|user|>'):\n",
    "            color = default_color\n",
    "        elif line.startswith('<|system|>'):\n",
    "            color = 'green'\n",
    "        if line.startswith('<'):\n",
    "            attrs = ['bold']\n",
    "        else:\n",
    "            attrs = None\n",
    "        print(colored(line, color, attrs=attrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'llama' in cfg.model_path:\n",
    "    # we need to add separation between numbers in the grid\n",
    "    grid_encoder = GridCodeBlockEncoder(GridWithSeparationEncoder('|'))\n",
    "else:\n",
    "    grid_encoder = GridCodeBlockEncoder(MinimalGridEncoder())\n",
    "train_dataset = create_dataset(cfg.train_dataset, grid_encoder, shuffle_question_order=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = create_dataset(cfg.val_dataset, grid_encoder, shuffle_question_order=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.adapter_path is None:\n",
    "    peft_config = LoraConfig(\n",
    "        # lora_alpha: LoRA scaling factor.\n",
    "        lora_alpha=64, #64,\n",
    "        lora_dropout=0.1, # 0.1, althought Vaca suggested to use 0.05 for big models\n",
    "        # r: the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
    "        r=32, #16\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        # target_modules: The modules (for example, attention blocks) to apply the LoRA update matrices.\n",
    "        target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj'],\n",
    "        use_rslora=cfg.use_rslora,\n",
    "        use_dora=cfg.use_dora,\n",
    "    )\n",
    "else:\n",
    "    print(f'Loading adapter from {cfg.adapter_path}')\n",
    "    peft_config = None\n",
    "    model = PeftModel.from_pretrained(model, cfg.adapter_path, is_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'llama' in cfg.model_path:\n",
    "    batch_size_kwargs = dict(\n",
    "        per_device_train_batch_size=3, # 4-16 should be fine for lora.\n",
    "        gradient_accumulation_steps=5,\n",
    "        per_device_eval_batch_size=4,\n",
    "    )\n",
    "else:\n",
    "    batch_size_kwargs = dict(\n",
    "        per_device_train_batch_size=8, # 4-16 should be fine for lora.\n",
    "        gradient_accumulation_steps=2,\n",
    "        per_device_eval_batch_size=8,\n",
    "    )\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "        output_dir=cfg.output_dir,\n",
    "        num_train_epochs=cfg.epochs,\n",
    "        warmup_ratio=cfg.warmup_ratio,\n",
    "        learning_rate=cfg.learning_rate,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_steps=cfg.eval_steps,\n",
    "        logging_steps=10, #50,\n",
    "        eval_steps=cfg.eval_steps,\n",
    "        log_level=\"debug\",\n",
    "\n",
    "        **batch_size_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'llama' in cfg.model_path:\n",
    "    data_collator = DataCollatorForCompletionOnlyLM(\n",
    "        tokenizer=tokenizer,\n",
    "        instruction_template='<|start_header_id|>user<|end_header_id|>',\n",
    "        response_template='<|start_header_id|>assistant<|end_header_id|>',\n",
    "    )\n",
    "else:\n",
    "    data_collator = DataCollatorForCompletionOnlyLM(\n",
    "        tokenizer=tokenizer,\n",
    "        instruction_template='<|user|>',\n",
    "        response_template='<|assistant|>'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = wandb.init(reinit=True,\n",
    "               dir=cfg.output_dir,\n",
    "               project=os.path.basename(os.path.dirname(cfg.output_dir)),\n",
    "               name=os.path.basename(cfg.output_dir))\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=cfg.max_seq_len,\n",
    "    data_collator=data_collator,\n",
    "    args=training_arguments,\n",
    "    # packing=True, # ValueError: You passed a `DataCollatorForCompletionOnlyLM` to the SFTTrainer. This is not compatible with the `packing` argument.\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "w.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluation for 1k samples is taking around 1 minute.\n",
    "- One epoch has taken around 30 minutes, with around 11 evaluations. Thus 1/3 of the time was spend evaluating. (550 steps, eval every 50 steps)\n",
    "- Training for 10 epochs took 5 hours. I set the temperature of the AC to 27ºC and the room was at 22ºC, probably 28ºC is fine.\n",
    "- Eval loss was 0.1665, without signs of overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "300 steps without flash attention: 16:17 minutes, with flash attention 14 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25 minutes to do 620 steps, around 10k samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://huggingface.co/docs/transformers/en/peft\n",
    "- https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.merge_and_unload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg.val_dataset, 'r') as f:\n",
    "    data = json.load(f)\n",
    "val_samples_ids = list(data.keys())\n",
    "\n",
    "def ask_question_to_model(sample_idx, pipe, question_idx=0, arbitrary_question=None):\n",
    "    sample_id = val_samples_ids[sample_idx]\n",
    "    sample = data[sample_id]\n",
    "\n",
    "    sample_with_one_question = sample.copy()\n",
    "    if arbitrary_question is None:\n",
    "        sample_with_one_question['questions'] = {question:answer for idx, (question, answer) in enumerate(sample['questions'].items()) if idx == question_idx}\n",
    "    else:\n",
    "        sample_with_one_question['questions'] = {arbitrary_question:''}\n",
    "\n",
    "    messages = create_messages_from_sample(sample_with_one_question, grid_encoder)\n",
    "    prompt = tokenizer.apply_chat_template(messages[:2],\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True)\n",
    "    plot_grid(sample['grid']); plt.show()\n",
    "    # pretty_print_prompt(prompt)\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"return_full_text\": False,\n",
    "        \"do_sample\": False,\n",
    "    }\n",
    "\n",
    "    output = pipe(prompt, **generation_args)\n",
    "    print(list(sample_with_one_question['questions'].keys())[0])\n",
    "    print(f\">{output[0]['generated_text']} ({list(sample_with_one_question['questions'].values())[0]})\")\n",
    "\n",
    "def plot_grid(grid):\n",
    "    grid = np.array(grid)\n",
    "    cmap = colors.ListedColormap(\n",
    "        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n",
    "         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
    "    norm = colors.Normalize(vmin=0, vmax=9)\n",
    "    plt.imshow(grid, cmap=cmap, norm=norm)\n",
    "    plt.grid(True,which='both',color='lightgrey', linewidth=0.5)\n",
    "    plt.xticks(np.arange(-0.5, grid.shape[1]), [])\n",
    "    plt.yticks(np.arange(-0.5, grid.shape[0]), [])\n",
    "    plt.xlim(-0.5, grid.shape[1]-0.5)\n",
    "\n",
    "    for i in range(grid.shape[0]):\n",
    "        for j in range(grid.shape[1]):\n",
    "            plt.text(j, i, grid[i, j], ha='center', va='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/11_lr_4e-4_1e5dataset_r32/checkpoint-12400'\n",
    "adapter_path = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/09_lr_1e-3_1e4dataset_r32/checkpoint-600/'\n",
    "adapter_path = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/14_llama31/checkpoint-333'\n",
    "adapter_path = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/15_continue_training_phi3_4e5/checkpoint-22800'\n",
    "adapter_path = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/22_random_question_lr1e-4_1e5dataset/checkpoint-6228'\n",
    "adapter_path = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/22_llama31_lr1e-4_1e5dataset_r32/checkpoint-6553'\n",
    "model.load_adapter(adapter_path, adapter_path)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to visualize the grid and ask some random question, see how well it does.\n",
    "\n",
    "Compare the responses with and without the adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question_to_model(sample_idx=750, question_idx=1, pipe=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask_question_to_model(sample_idx=300, arbitrary_question='Please describe the grid, saying how many objects are there, their color and area.', pipe=pipe)\n",
    "# ask_question_to_model(sample_idx=300, arbitrary_question='What is the shape of the grid? (nxn)', pipe=pipe)\n",
    "ask_question_to_model(sample_idx=550, arbitrary_question='Describe the objects in the grid', pipe=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Fixed val dataset\n",
    "- [x] The pad_token_id and eos_token_id values of this tokenizer are identical. If you are planning for multi-turn training, it can result in the model continuously generating questions and answers without eos token. To avoid this, set the pad_token_id to a different value. **I have evaluated phi-3 and works correctly**\n",
    "- [x] Flash attention? Yes, it is slightly faster.\n",
    "- [x] Batch size\n",
    "- [x] Better WANDB configuration\n",
    "- [x] Training parameters?\n",
    "  - [x] Learning rate\n",
    "  - [x] Is the linear decay working properly? Yes\n",
    "- [x] Better switch between Llama and Phi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
